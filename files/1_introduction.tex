\chapter{Introduction}
\label{ch1_introduction}
\section{Motivation}

Heterogeneous computing platforms with graphics processing units (GPUs) and massively parallel processor arrays (MPPAs) acting as co-processors, help in offloading the computational workload, hence giving improvement in performance in terms of energy efficiency and resource utilization. However the amount of performance gain depends significantly on the customization of the hardware accelerator based on the algorithm. Depending on the algorithm, specific accelerators may perform better than other accelerators. Hence running an algorithm efficiently is ideally a mapping an algorithm to architecture problem. The challenge in this process is to identify the portions of the applications which will give speedup on hardware under resource constraints. Usually the identification is based on the execution times or computation requirements. 

When hardware accelerators are used, certain modifications specific to the accelerator help in getting performance benefits. e.g. in FPGA, datatypes and bit widths have significant impact on the performance. e.g. Single precision floating point datatype consumes lesser number of resources than double precision floating point datatype. Similarly fixed point computation units require lesser number of resources than floating point computation units, which can be used to increase number of concurrent executions of an algorithm and hence increasing in parallelism which may help to gain speedup. However these modifications directly impact the correctness of the algorithm. This report aims to explore these trade-offs by analyzing two compute-intensive applications. The first one is Fully Homomorphic Encryption (FHE) and the second one is Convolution Neural Network (CNN) used for digit classification problem.

The first part of the report presents the result of modifying the underlying arithmetic from double precision float to single precision float for FHE application. We observe a significant loss in the accuracy of the application by changing the arithmetic and decide to explore the double precision float based FFT hardware generated by high level synthesis (HLS)
tool. We perform the design space exploration for double precision floating point FFT hardware.

The second part of the report presents the result of modifying the arithmetic from single precision float to low precision fixed point for CNN. An important step in design flow of computation intensive embedded system applications is conversion from float to fixed point representation. Although float offers more dynamic range, it is expensive to compute. However some applications can tolerate a certain level of inaccuracy with usage of simple float representation. 
An existing open source implementation of CNN that classifies MNIST Data set is used and the code is modified to use fixed point representation instead of single precision float. The effect on accuracy of the results is compared and analyzed for different fixed point formats. We generate the convolution engine using HLS tool and show the improvement in latency and resource utilization by using fixed point arithmetic. 



\section{Contribution}
The main contributions can be summarized as follows:

\vspace{0.25cm}
\textbf{Fully Homomorphic Encryption (FHE)}
\begin{itemize}
\item Understanding the algorithm and the existing implementation to identify the computationally intense modules of the application. 
\item Modifying the underlying arithmetic from double precision float to single precision float for FHE application and studying the affect on accuracy.
\item Bottom-up analysis of fast fourier transform, starting from a single butterfly unit, to yield a resource optimized implementation of 1024-point FFT.
\end{itemize}

\textbf{Convolutional Neural Network (CNN)}
\begin{itemize}
\item Modification of the existing implementation to support fixed point arithmetic using libfi library.
\item Analyzing the error introduced due to low precision fixed point implementation, by varying the number of bits used for fixed point format. 
\item Estimating the improvement in area and latency by implementing the convolution engine using Vivado HLS. 
\end{itemize}

\section{Organization}

The remainder of the report is organized as follows:
Chapter \ref{Chapter2} gives background information on fully homomorphic encryption (FHE), convolutional neural network and basics of floating point and fixed point arithmetic.
Chapter \ref{Chapter3} presents the modification in the arithmetic of FHE from double precision floating point to single precision floating point and a thorough analysis of the results.
Chapter \ref{Chapter4} discusses the implementation of an area optimized 1024 point FFT (Fast Fourier Transform) on Vivado HLS, starting from the analysis of smallest butterfly compute unit.
Chapter \ref{Chapter5} discusses the existing implementation of lenet model for classification of MNIST dataset. It also discusses the mathematics of fixed point briefly and a library \textit{Libfi} which is used to integrate fixed point mathematics with the above mentioned implementation of lenet model. It also tabulates the results of the modification on accuracy, area and latency.  
Chapter \ref{Chapter6} summarizes the results of the analysis and suggests areas for future work.







